
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>inference推理 &#8212; dig-into-apollo  documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/alabaster.css" />
    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
   
  <link rel="stylesheet" href="../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <a name="inference_module" />
<div class="section" id="inference">
<h1>inference推理<a class="headerlink" href="#inference" title="Permalink to this headline">¶</a></h1>
<p>深度学习模型包括训练和推理2个过程，训练模型的过程是通过设计神经网络，通过数据训练出模型的参数。而推理则是部署深度学习的过程，实际上目前训练深度学习主要用的是python语言，而部署的时候大部分会采用c++，并且通过gpu进行加速，而inference模块则实现了上述功能。</p>
<p>inference主要实现了tensorflow, caffe, paddlepaddle3种框架的实现，并且通过cuda进行计算加速。</p>
<div class="section" id="caffe">
<h2>caffe<a class="headerlink" href="#caffe" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="paddlepaddle">
<h2>paddlepaddle<a class="headerlink" href="#paddlepaddle" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="tensorrt">
<h2>tensorrt<a class="headerlink" href="#tensorrt" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="id1">
<h2>目录介绍<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>├── BUILD
├── caffe // caffe框架
├── inference.cc // 定义了推理接口
├── inference_factory.cc // 推理工厂，用来创建推理器
├── inference_factory.h
├── inference_factory_test.cc
├── inference.h
├── inference_test.cc
├── inference_test_data  // 推理测试数据
├── layer.cc // 定义了layer接口
├── layer.h
├── layer_test.cc
├── operators // 算子？？？
├── paddlepaddle  // paddlepaddle框架
├── tensorrt // tensorrt框架
├── test
├── tools // yolo,lane等的例子
└── utils // cuda加速工具
</pre></div>
</div>
</div>
<div class="section" id="createinferencebyname">
<h2>CreateInferenceByName<a class="headerlink" href="#createinferencebyname" title="Permalink to this headline">¶</a></h2>
<p>CreateInferenceByName可以创建3种形式的推理器caffe, paddlepaddle, tensorrt。这里的tensorrt就是英伟达的加速库吗？也就是说如果是其它模型则采用tensorrt部署，caffe和paddlepaddle则采用这2种高级别的api部署？</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">Inference</span><span class="w"> </span><span class="o">*</span><span class="nf">CreateInferenceByName</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="o">&amp;</span><span class="n">name</span><span class="p">,</span><span class="w"></span>
<span class="w">                                 </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="o">&amp;</span><span class="n">proto_file</span><span class="p">,</span><span class="w"></span>
<span class="w">                                 </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="o">&amp;</span><span class="n">weight_file</span><span class="p">,</span><span class="w"></span>
<span class="w">                                 </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">,</span><span class="w"></span>
<span class="w">                                 </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">inputs</span><span class="p">,</span><span class="w"></span>
<span class="w">                                 </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="o">&amp;</span><span class="n">model_root</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">name</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s">&quot;CaffeNet&quot;</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">CaffeNet</span><span class="p">(</span><span class="n">proto_file</span><span class="p">,</span><span class="w"> </span><span class="n">weight_file</span><span class="p">,</span><span class="w"> </span><span class="n">outputs</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="p">);</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">name</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s">&quot;RTNet&quot;</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">RTNet</span><span class="p">(</span><span class="n">proto_file</span><span class="p">,</span><span class="w"> </span><span class="n">weight_file</span><span class="p">,</span><span class="w"> </span><span class="n">outputs</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="p">);</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">name</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s">&quot;RTNetInt8&quot;</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">RTNet</span><span class="p">(</span><span class="n">proto_file</span><span class="p">,</span><span class="w"> </span><span class="n">weight_file</span><span class="p">,</span><span class="w"> </span><span class="n">outputs</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="n">model_root</span><span class="p">);</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">name</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s">&quot;PaddleNet&quot;</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">PaddleNet</span><span class="p">(</span><span class="n">proto_file</span><span class="p">,</span><span class="w"> </span><span class="n">weight_file</span><span class="p">,</span><span class="w"> </span><span class="n">outputs</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="p">);</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
<p>3种推理模型分别在caffe, paddlepaddle, tensorrt目录中，其中有用到cuda进行加速，其中”.cu”是cuda对C++的扩展。</p>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../index.html">dig-into-apollo</a></h1>








<h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../cyber/readme.html">Dig into Apollo - Cyber </a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../docker/readme.html">Dig into Apollo - Docker </a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2022, daohu527.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 4.5.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../../../_sources/modules/perception/inference/readme.md.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>